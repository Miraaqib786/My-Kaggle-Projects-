{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nIn this notebook, we will explore various techniques and models for Natural Language Processing (NLP). We will cover topics such as text preprocessing, feature extraction, sentiment analysis, and text classification.\n\n## Dataset\nFor this project, we will use the \"Sentiment Analysis on Movie Reviews\" dataset from Kaggle. It contains movie reviews along with their corresponding sentiment labels (positive or negative).\n\n## Data Preprocessing\n1. Load the dataset and perform basic exploratory data analysis.\n2. Clean the text data by removing special characters, numbers, and stopwords.\n3. Perform tokenization and lemmatization to convert text into a usable format.\n\n## Feature Extraction\n1. Implement Bag-of-Words (BoW) representation using CountVectorizer or TfidfVectorizer.\n2. Generate word embeddings using pre-trained models such as Word2Vec or GloVe.\n3. Explore feature engineering techniques like n-grams, term frequency-inverse document frequency (TF-IDF), and word frequency.\n\n## Sentiment Analysis\n1. Split the dataset into training and testing sets.\n2. Train a sentiment analysis model using various algorithms like Naive Bayes, Support Vector Machines (SVM), or Recurrent Neural Networks (RNNs).\n3. Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score.\n\n## Text Classification\n1. Convert the text data into numerical features using the chosen feature extraction technique.\n2. Split the dataset into training and testing sets.\n3. Train a text classification model using algorithms like Logistic Regression, Random Forest, or Convolutional Neural Networks (CNNs).\n4. Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score.\n\n## Conclusion\nNLP is a fascinating field that offers a wide range of techniques for understanding and analyzing textual data. In this notebook, we explored various aspects of NLP, including data preprocessing, feature extraction, sentiment analysis, and text classification. By leveraging these techniques, we can gain valuable insights from text data and build powerful predictive models.\n","metadata":{}},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# Load the dataset\nimport pandas as pd\ndf = pd.read_csv(\"movie_reviews.csv\")\n\n# Basic exploratory data analysis\nprint(df.head())\nprint(df.shape)\n\n# Clean the text data\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\ndef clean_text(text):\n    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n    text = text.lower()\n    text = word_tokenize(text)\n    text = [word for word in text if word not in stopwords.words(\"english\")]\n    lemmatizer = WordNetLemmatizer()\n    text = [lemmatizer.lemmatize(word) for word in text]\n    return \" \".join(text)\n\ndf[\"cleaned_text\"] = df[\"text\"].apply(clean_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Extraction(Bag od Words)**","metadata":{}},{"cell_type":"code","source":"# Implement Bag-of-Words representation\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(df[\"cleaned_text\"])\ny = df[\"sentiment\"]\n\n# Print the feature matrix shape\nprint(X.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Sentiment Analysis (Naive Bayes)**","metadata":{}},{"cell_type":"code","source":"# Split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\n\nnb_model = MultinomialNB()\nnb_model.fit(X_train, y_train)\n\n# Evaluate the model's performance\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = nb_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-Score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Text Classification (Logistic Regression)**","metadata":{}},{"cell_type":"code","source":"# Convert text data into numerical features using Bag-of-Words representation\nX = vectorizer.transform(df[\"cleaned_text\"])\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\nlr_model.fit(X_train, y_train)\n\n# Evaluate the model's performance\ny_pred = lr_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-Score:\", f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Data Analysis (Word Cloud)**","metadata":{}},{"cell_type":"code","source":"# Visualize word cloud for positive sentiment\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\npositive_reviews = df[df[\"sentiment\"] == \"positive\"][\"cleaned_text\"]\npositive_text = \" \".join(positive_reviews)\n\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(positive_text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Word Cloud - Positive Sentiment\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Performance Evaluation (Confusion Matrix)**","metadata":{}},{"cell_type":"code","source":"# Evaluate the model's performance using a confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Performance Comparison (Bar Plot)**","metadata":{}},{"cell_type":"code","source":"# Compare model performance using bar plot\nmodels = [\"Naive Bayes\", \"Logistic Regression\"]\naccuracy_scores = [accuracy_nb, accuracy_lr]\nprecision_scores = [precision_nb, precision_lr]\nrecall_scores = [recall_nb, recall_lr]\nf1_scores = [f1_nb, f1_lr]\n\nplt.figure(figsize=(10, 6))\nx_pos = [i for i, _ in enumerate(models)]\nplt.bar(x_pos, accuracy_scores, color=\"blue\", alpha=0.7, label=\"Accuracy\")\nplt.bar(x_pos, precision_scores, color=\"green\", alpha=0.7, label=\"Precision\")\nplt.bar(x_pos, recall_scores, color=\"orange\", alpha=0.7, label=\"Recall\")\nplt.bar(x_pos, f1_scores, color=\"purple\", alpha=0.7, label=\"F1-Score\")\nplt.xticks(x_pos, models)\nplt.xlabel(\"Model\")\nplt.ylabel(\"Scores\")\nplt.title(\"Model Performance Comparison\")\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}